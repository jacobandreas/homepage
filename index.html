<html>
<head lang="en">
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Jacob Andreas @ MIT</title>
  <link href="css/bootstrap.min.css" rel="stylesheet">
  <link href="css/style.css" rel="stylesheet">
  <link href="css/research.css" rel="stylesheet">
</head>
<body>
<div class="content">
  <h2 class="name">Jacob Andreas</h2>
  <img class="margin" src="figs/head_small.jpg" width="100%">

  <p>
    <b>I'm interested in language as a communicative and computational tool.</b>
    People learn to understand and generate novel utterances from remarkably
    little data. Having learned language, we use it acquire new concepts and to
    structure our reasoning. Current machine learning techniques fall short of
    human abilities in both their capacity to <i>learn language</i> and <i>learn
    from language</i> about the rest of the world. My research aims to (1)
    understand the computational foundations of efficient language learning,
    and (2) build general-purpose intelligent systems that can communicate
    effectively with humans and learn from human guidance.
  </p>

  <p>
    I'm an associate professor at MIT in 
    <a href="http://www.eecs.mit.edu/">EECS</a> and
    <a href="http://www.eecs.mit.edu/">CSAIL</a>.
    I did my PhD work at Berkeley, where I was a member of the 
    <a href='http://nlp.cs.berkeley.edu/'>Berkeley NLP Group</a>
    and the 
    <a href="http://bair.berkeley.edu/">Berkeley AI Research Lab</a>.  
    I've also spent time with the
    <a href="http://www.cl.cam.ac.uk/research/nl/"> Cambridge NLIP Group</a>, 
    and the
    <a href="http://www.cs.columbia.edu/nlp">NLP Group</a>
    and the (erstwhile)
    Center for Computational Learning Systems
    at Columbia.
  </p>

  <p>
    <strong>Prospective students and visitors</strong>: please see the contact
    page below. Read my <a
    href="advising.html">advising statement</a> if you're considering applying!
  </p>

  <p>
    <a href='docs/jda_cv.pdf'>Curriculum vit&aelig;</a>,
    <a href='http://scholar.google.com/citations?user=dnZ8udEAAAAJ'>Google scholar</a>,
    <a href='http://accessibility.mit.edu'>accessibility @ MIT</a>
  </p>

  <hr/>

  <h3>
    <a href="contact.html">Contact</a> /
    <a href="http://lingo.csail.mit.edu">Group</a> /
    <a href="http://lingo.csail.mit.edu/research.html">Research</a> /
    <a href="bio.html">Bio</a> /
    <a href="teaching/">Teaching</a>
  </h3>

  <p>
  </p>

  <hr/>

  Some current research directions:

  <div class="highlight">
    <!--<img src="figs/l3.jpg" class="margin">-->
    <h4>Learning from language</h4>
    <p>
      Much of what humans know (and know how to do) comes not from
      observation, but rich supervision provided in language by skilled
      teachers.  But almost all machine learning research focuses on learning
      from comparatively low-level demonstrations or interactions. How do we
      enable more natural and efficient learning from natural language
      supervision instead?
    </p>
    <p>
      <b>Papers</b>:<br/>
      <a href="https://arxiv.org/abs/2401.08574">Deductive closure training</a> (preprint)<br/>
      <a href="https://arxiv.org/abs/2310.11589">Eliciting human preferences with language models</a> (preprint)<br/>
      <a href="https://arxiv.org/abs/2312.08566">Learning adaptive planning representations with natural langauge guidance</a> (ICLR 2024)<br/>
      <a href="https://arxiv.org/abs/2110.01517">Skill induction and planning with latent language</a> (ACL 2022)<br/>
    </p>
  </div>

  <div class="highlight">
    <!--<img src="figs/neuralese.jpg" class="margin">-->
    <h4>Automatic interpretation and explanation of learned models</h4>
    <p>
      What tools do we need to help humans understand the features and
      representational strategies that black-box machine learning algorithms
      discover? To what extent do these strategies reflect abstractions that we
      already have names for?
    </p>
    <p>
      <b>Papers</b>:<br/>
      <a href="https://arxiv.org/abs/2211.15661">What learning algorithm is in-context learning? Investigations with linear models</a> (ICLR 2023)<br/>
      <a href="https://arxiv.org/abs/2201.11114">Natural language descriptions of deep visual features</a> (ICLR 2022)<br/>
      <a href="https://arxiv.org/abs/2106.00737">Implicit representations of meaning in neural language models</a> (ACL 2021)<br/>
    </p>
  </div>

  <div class="highlight">
    <!--<img src="figs/compositionality.jpg" class="margin">-->
    <h4>Human-like language understanding</h4>
    <p>
      Humans learn language much faster&mdash;and employ it much more
      flexibly&mdash;than even the most sophisticated language models that exist
      today. How can we use computational models to understand the algorithms
      and inductive biases that support human language learning and
      comprehension? How do we use these to build better language processing
      systems?
    </p>
    <p>
      <b>Papers</b>:<br/>
      <a href="https://arxiv.org/abs/2311.09712">Regularized conventions:
        equilibrium computation as a model of pragmatic reasoning</a> (NAACL
      2024) <br/>
      <a href="https://arxiv.org/abs/2201.12926">Compositionality as lexical symmetry</a> (ACL 2023)</br>
      <a href="https://arxiv.org/abs/2211.01288">Characterizing intrinsic compositionality in transformers with tree projections</a> (ICLR 2023)<br/>
    </p>
  </div>

  <p>
    I'm also interested in
    <a href="https://arxiv.org/abs/1705.03919">trees</a>,
    <a href="https://www.aclweb.org/anthology/P13-1091/">graphs</a>, 
    <a href="https://arxiv.org/abs/1604.00562">games</a>,
    and
    <a href="https://papers.nips.cc/paper/5432-unsupervised-transcription-of-piano-music">sounds</a>.
  </p>

  <hr/>

  <p class='offset'>
    Collaboration graph trivia: My Erd&#337;s number is at most three 
    (J Andreas <a href="https://arxiv.org/abs/1711.02301">to</a> 
    R Kleinberg <a href="https://dl.acm.org/citation.cfm?id=1255444">to</a> 
    L Lov&aacute;sz <a href="https://www.sciencedirect.com/science/article/pii/B9780720422627500181">to</a>
    P Erd&#337;s). 
    My Kevin Bacon number (and consequently my Erd&#337;s-Bacon number) remains
    <a href="http://jacobandreas.net/misc/bacon.html">lamentably undefined</a>,
    but my Kevin Knight number (it's
    <a href="https://u.cs.biu.ac.il/~yogo/">a thing</a>)
    is one. I have never starred in a film with Kevin Knight. Noam Chomsky is
    my great-great-grand-advisor (J Andreas to D Klein to C Manning to J
    Bresnan to N Chomsky).
  </p>

  <hr/>

  <p class="credit">Photo: Gretchen Ertl / MIT CSAIL</p>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
</div>
</body>
</html>
