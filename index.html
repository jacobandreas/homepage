<html>
<head lang="en">
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Jacob Andreas @ MIT</title>
  <link href="css/bootstrap.min.css" rel="stylesheet">
  <link href="css/style.css" rel="stylesheet">
  <link href="css/research.css" rel="stylesheet">
</head>
<body>
<div class="content">
  <h2 class="name">Jacob Andreas</h2>
  <img src="figs/face2.jpg" class="margin">

  <p>
    I'm interested in what machine learning can teach us about natural language
    (especially about compositionality and generalization), and in
    how we can use language as a tool for building general-purpose intelligent
    systems (e.g. as a source of explanations or a scaffold for efficient
    learning).
  </p>

  <p>
    I'm an assistant professor at MIT in <a
    href="http://www.eecs.mit.edu/">EECS</a> and <a
    href="http://www.eecs.mit.edu/">CSAIL</a>. (I'm also affiliated with
    Microsoft <a href="http://www.semanticmachines.com/">Semantic Machines</a>.) 
    I earned my PhD at Berkeley, where I was a member of the 
    <a href='http://nlp.cs.berkeley.edu/'>Berkeley NLP Group</a>
    and the 
    <a href="http://bair.berkeley.edu/">Berkeley AI Research Lab</a>.  
    Previously I worked with the 
    <a href="http://www.cl.cam.ac.uk/research/nl/"> Cambridge NLIP Group</a>, 
    and the 
    <a href="http://ccls.columbia.edu">Center for Computational Learning Systems</a>
    and
    <a href="http://www.cs.columbia.edu/nlp">NLP Group</a>
    at Columbia.
  </p>

  <p>
    <strong>Prospective students</strong>: apply through the MIT 
    <a href="https://gradapply.mit.edu/eecs/apply/login/?next=/eecs/">
    graduate admissions portal</a> in the fall. (I'm afraid I can't respond to
    emails individually.)
  </p>

  <p>
    <a href='mailto:jda@cs.berkeley.edu'>jda@cs.berkeley.edu</a>,
    <a href='docs/jda_cv.pdf'>Curriculum vit&aelig;</a>,
    <a href='http://scholar.google.com/citations?user=dnZ8udEAAAAJ'>Google scholar</a>,
    <a href="http://jacobandreas.net">elsewhere</a>
  </p>

  <hr/>

  <h3><a href="talks.html">Talks</a> / <a href="pubs.html">Publications</a></h3>

  <hr/>

  Some current research directions:

  <div class="highlight">
    <h4>Composable machine learning</h4>
    <p>
      Compositionality and modularity are core features of representational
      systems in language, software and biology. What do we gain from
      explicitly encouraging compositional structure? Can we use descriptions of
      abstract compositional structure in one domain (e.g. language) to learn
      modular representations in another (e.g. vision)?
    </p>
    <p>
      <b>Papers</b>:<br/>
      <a href="https://arxiv.org/pdf/1611.01796">
        Modular multitask reinforcement learning with policy sketches
      </a> (ICML 2017)<br/>
      <a href="https://arxiv.org/abs/1511.02799">
        Neural module networks
      </a> (CVPR 2016)<br/>
    </p>
  </div>

  <div class="highlight">
    <h4>Explaining black-box representations</h4>
    <p>
      How can we help humans understand the features and representational
      strategies that black-box learning algorithms discover? To what extent to
      these already reflect abstractions described by natural language?
    </p>
    <p>
      <b>Papers</b>:<br/>
      <a href="https://arxiv.org/abs/1902.07181">
        Measuring compositionality in representation learning
      </a> (ICLR 2019)</br>
      <a href="https://arxiv.org/pdf/1704.06960">
        Translating neuralese
      </a> (ACL 2017)<br/>
      <a href="https://arxiv.org/pdf/1707.08139">
        Analogs of linguistic structure in deep representations
      </a> (EMNLP 2017)

    </p>
  </div>

  <p>
    I'm also interested in
    <a href="http://arxiv.org/pdf/1604.0056">pragmatics</a>,
    <a href="https://www.aclweb.org/anthology/P13-1091">graph automata</a>, 
    and
    <a href="https://papers.nips.cc/paper/5432-unsupervised-transcription-of-piano-music">music</a>.
  </p>

  <hr/>

  <p class='offset'>
    Collaboration graph trivia: My Erd&#337;s number is at most three 
    (J Andreas <a href="https://arxiv.org/abs/1711.02301">to</a> 
    R Kleinberg <a href="https://dl.acm.org/citation.cfm?id=1255444">to</a> 
    L Lov&aacute;sz <a href="https://www.sciencedirect.com/science/article/pii/B9780720422627500181">to</a>
    P Erd&#337;s). 
    My Kevin Bacon number (and consequently my Erd&#337;s-Bacon number) remains
    <a href="http://jacobandreas.net/misc/bacon.html">lamentably undefined</a>,
    but my Kevin Knight number (since apparently that's 
    <a href='http://www.cs.bgu.ac.il/~yoavg/uni/'>a thing</a>) 
    is one. I have never starred in a film with Kevin Knight.  Noam Chomsky is
    my great-great-grand-advisor (J Andreas to D Klein to C Manning to J
    Bresnan to N Chomsky).
  </p>

  <hr/>

  <p>
    Annotated bibliographies on:<br/>
    <a href='https://github.com/jacobandreas/bibs/blob/master/nmn.md'>
      module networks</a><br/>
    <a href='https://github.com/jacobandreas/bibs/blob/master/language_behavior.md'>
      language and behavior</a><br/>
  <p>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
</div>
</body>
</html>
